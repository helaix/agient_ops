# Claude Opus 4 Prompting Research Evaluation Criteria

## Research Quality Standards

### Source Credibility (Required)
- **Minimum 3 authoritative sources per technique**
- Acceptable source types:
  - Anthropic official documentation and research papers
  - Peer-reviewed academic papers on LLM prompting
  - Established AI research organizations (OpenAI, Google DeepMind, etc.)
  - Reputable AI/ML conferences and journals
  - Well-documented community research with reproducible results

### Evidence Requirements
- **Performance Data**: Quantitative metrics where available
- **Reproducibility**: Clear implementation examples
- **Validation**: Multiple independent confirmations of effectiveness
- **Recency**: Preference for sources from 2023-2024 (Claude Opus 4 era)

## Technique Evaluation Framework

### Effectiveness Metrics
1. **Accuracy Improvement**
   - Measurable improvement in task completion
   - Reduction in hallucinations or errors
   - Consistency across multiple runs

2. **Output Quality**
   - Relevance to user intent
   - Coherence and structure
   - Depth and comprehensiveness

3. **Efficiency**
   - Token usage optimization
   - Response time considerations
   - Cost-effectiveness

### Categorization Standards

#### By Complexity Level
- **Basic**: Simple, single-step techniques
- **Intermediate**: Multi-step or conditional approaches
- **Advanced**: Complex, context-dependent strategies

#### By Use Case
- **General Purpose**: Broadly applicable techniques
- **Domain-Specific**: Specialized for particular fields
- **Task-Specific**: Optimized for specific types of requests

#### By Implementation Scope
- **Prompt Engineering**: Direct prompt modifications
- **Context Management**: Information organization strategies
- **Output Formatting**: Response structure optimization
- **Chain-of-Thought**: Reasoning enhancement techniques

## Documentation Standards

### Required Elements
- [ ] Clear technique description
- [ ] Implementation examples (basic and advanced)
- [ ] Evidence from minimum 3 sources
- [ ] Performance metrics or qualitative assessments
- [ ] Limitations and considerations
- [ ] Best practices checklist

### Quality Indicators
- **Specificity**: Concrete examples rather than abstract descriptions
- **Actionability**: Clear implementation guidance
- **Completeness**: All template sections filled with relevant content
- **Accuracy**: Factual correctness and proper attribution

## Research Methodology

### Search Strategy
1. **Primary Sources**: Anthropic documentation, research papers
2. **Academic Sources**: arXiv, ACL, NeurIPS, ICML proceedings
3. **Industry Sources**: AI company blogs, technical documentation
4. **Community Sources**: Validated community research and benchmarks

### Validation Process
1. **Cross-Reference**: Verify findings across multiple sources
2. **Test Examples**: Validate provided examples work as described
3. **Context Check**: Ensure relevance to Claude Opus 4 specifically
4. **Recency Verification**: Confirm information is current and applicable

## Success Criteria

### Minimum Requirements
- [ ] 10+ distinct prompting techniques documented
- [ ] Each technique has 3+ authoritative sources
- [ ] Clear implementation examples for each technique
- [ ] Evidence-based performance claims
- [ ] Comprehensive coverage of major use cases

### Excellence Indicators
- [ ] 15+ techniques with comprehensive documentation
- [ ] Quantitative performance data where available
- [ ] Novel or recently discovered techniques included
- [ ] Clear categorization and cross-referencing
- [ ] Practical implementation guidance and troubleshooting

